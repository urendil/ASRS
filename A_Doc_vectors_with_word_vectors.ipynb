{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook A : Doc vectors with word vectors\n",
    "\n",
    "Began 21 May 2020 by Amaury de Barbuat from ECL\n",
    "\n",
    "Updated by William Riou from ENSTA PARIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0- Intro\n",
    "\n",
    "The goal is to do document embed for each pilot report using word vectors averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Create word embeddings\n",
    "\n",
    "We will do word embeddings over a given report basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entry : csv file as 'time', 'content', 'title'\n",
    "# output : line sentence cleaned text file\n",
    "\n",
    "def process_csv(input_filename, output_filename, reports_filename):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import os.path\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "    data = pd.read_csv(input_filename, sep=',')\n",
    "    \n",
    "    final = pd.DataFrame(columns=['reports'])\n",
    "    \n",
    "    MyFile = open(output_filename,'w')\n",
    "    if ~os.path.isfile('abbr.txt'):\n",
    "        # abbreviation\n",
    "        file = open('abbr.txt', 'w')\n",
    "        file.close()\n",
    "    \n",
    "    file = open('abbr.txt','r')\n",
    "    abbrs = file.readlines()\n",
    "    file.close()\n",
    "    old = [] \n",
    "    new = []\n",
    "    for abbr in abbrs :\n",
    "        abbr = abbr.split()\n",
    "        old.append(abbr[0])\n",
    "        new.append(abbr[1])\n",
    "\n",
    "    for i in range (len(data)):\n",
    "        \n",
    "        if data['time'][i]=='2008-01-01': #or data['time'][i][:4]=='2006' or data['time'][i][:4]=='2007':\n",
    "        \n",
    "            report = str(data['title'][i])+' '+str(data['content'][i])\n",
    "            from nltk import sent_tokenize\n",
    "            sentences = sent_tokenize(report)\n",
    "\n",
    "            new_report=''\n",
    "\n",
    "            for s in sentences:\n",
    "\n",
    "                line=''\n",
    "\n",
    "                from nltk.tokenize import word_tokenize\n",
    "                tokens = word_tokenize(s)\n",
    "\n",
    "                # convert to lower case\n",
    "                tokens = [w.lower() for w in tokens]\n",
    "\n",
    "                # remove punctuation from each word\n",
    "                import string\n",
    "                table = str.maketrans('', '', string.punctuation)\n",
    "                stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "                # remove remaining tokens that are not alphabetic\n",
    "                #words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "                # remove abbreviations\n",
    "                words_wo_abbrs=[]\n",
    "                for word in stripped:\n",
    "                    if word in old:\n",
    "                        u=old.index(word)\n",
    "                        words_wo_abbrs.append(new[u])\n",
    "                    else:\n",
    "                        words_wo_abbrs.append(word)\n",
    "\n",
    "                # filter out stop words\n",
    "                #from nltk.corpus import stopwords\n",
    "                #stop_words = set(stopwords.words('french'))\n",
    "                #temp_words = [w for w in words_wo_abbrs if not w in stop_words]\n",
    "\n",
    "                # stemming\n",
    "                #from nltk.stem.porter import PorterStemmer\n",
    "                #porter = PorterStemmer()\n",
    "                #final_words = [porter.stem(word) for word in temp_words]\n",
    "\n",
    "                for word in words_wo_abbrs:\n",
    "                    line += word+' '\n",
    "\n",
    "                MyFile.write(line+'\\n')\n",
    "\n",
    "                new_report += line+'. '\n",
    "\n",
    "            final.loc[i] = [new_report]\n",
    "        \n",
    "    final.to_csv(reports_filename, sep=',', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/urendil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "process_csv('ASRS1.csv', 'LineSentences_57.txt', 'reports_57.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_lines(input_filename, output_filename):\n",
    "    \n",
    "    import random\n",
    "    lines = open(input_filename, 'r').readlines()\n",
    "    random.shuffle(lines)\n",
    "    open(output_filename, 'w').writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_lines('LineSentences_57.txt', 'LineSentences_shuffled_57.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entry : cleaned txt file with one sentence per line\n",
    "# output : KeyedVector .kv file in tmp file containing the embedding\n",
    "\n",
    "def word_embedding(input_filename, vectors_filename):\n",
    "    \n",
    "    # Create the model\n",
    "    import os\n",
    "    from gensim.models.word2vec import Word2Vec\n",
    "    from gensim.models.word2vec import LineSentence\n",
    "    sentences = LineSentence(input_filename, max_sentence_length=10000, limit=None)\n",
    "    model = Word2Vec(sentences, min_count=5, size=100, iter=5)\n",
    "    \n",
    "    # Save model\n",
    "    from gensim.models.keyedvectors import KeyedVectors\n",
    "    model.wv.save_word2vec_format(os.path.join(os.getcwd(),vectors_filename), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding('LineSentences_shuffled_57.txt', 'word_vectors_57_100d_5e.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Document embeddings with word vectors averaging\n",
    "\n",
    "Here we want to embed each report by averaging its word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2vtxt_to_lists_of_words_and_vectors(filename):\n",
    "\n",
    "    file = open(filename, \"r\")\n",
    "    headers = next(file) # in txt w2v format, first line contains number of words and and vectors dimension\n",
    "    lines = file.readlines()\n",
    "    num_vect = int(headers.split()[0]) # number of words in the file\n",
    "    dim_vect = int(headers.split()[1]) # dimension of word vectors\n",
    "    file.close()\n",
    "\n",
    "    vectors = [] # will contain every vectors as sublists e.g. [[0.13, ..., -0.87], [-0.45, ..., 0.02], ...]\n",
    "    words = [] # will contain every words e.g. ['cat', 'dog', ...]\n",
    "\n",
    "    for line in lines :\n",
    "        line = line.split() # turn into list each line, they contain the word and its n dimensions\n",
    "        vector = line[1:(dim_vect+1)] # coordinates are index 1 to dim_vect+1\n",
    "        vector = [float(i) for i in vector]\n",
    "        word = line[0] # the corresponding word is the first element of the line\n",
    "        vectors.append(vector)\n",
    "        words.append(word)\n",
    "    \n",
    "    return (vectors, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc_vectors(reports_filename, word_vectors, doc_vectors):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    data = pd.read_csv(reports_filename, sep=',')\n",
    "    \n",
    "    MyFile = open(doc_vectors,'w')\n",
    "    \n",
    "    s=' '\n",
    "    \n",
    "    vectors, words = w2vtxt_to_lists_of_words_and_vectors(word_vectors)\n",
    "    \n",
    "    for i in range (len(data)):\n",
    "        \n",
    "        report = data['reports'][i].split(' ')\n",
    "        \n",
    "        avg = np.array([0.0 for k in range (len(vectors[0]))])\n",
    "        n_words = 0\n",
    "        for e in report:\n",
    "            if e in words:\n",
    "                ind = words.index(e)\n",
    "                avg += np.array(vectors[ind])\n",
    "                n_words += 1\n",
    "        \n",
    "        report_vector = avg/n_words\n",
    "        report_vector = report_vector.tolist()\n",
    "        report_vector = [str(e) for e in report_vector]\n",
    "        report_vector = s.join(report_vector)\n",
    "        \n",
    "        MyFile.write(report_vector+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_doc_vectors('reports_57.csv', 'word_vectors_57_100d_5e.txt', 'w2v_doc_vectors_57_100d_5e.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
