{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 : Dataset Preprocessing\n",
    "\n",
    "Began 3 March 2020 for 2 weeks by Amaury de Barbuat from ECL\n",
    "\n",
    "Updated by William Riou from ENSTA PARIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Intro\n",
    "\n",
    "The goal of this Notebook is to do multiple modifications of the initial dataset, using pandas library to manipulate Dataframes, in order to use the file on SumUp Nucleus. To do so, the final goal is to get a file with the following columns : ['Title','Content','Time'] where title refers to synopsis in TEXT.csv, content refers to narrative in TEXT.csv and time refers to date in ALL_ITEMS.csv.\n",
    "\n",
    "This process is very spacedisk and time consumer. It is not optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic useful libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing of TEXT.csv\n",
    "\n",
    "File requires several modifications to be usable for step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11- N1/2/3/4/5 and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/urendil/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# First mods detailed as following :\n",
    "# a. Rows with 2 cells turned into 1. The writer used a \";\" in is report wich creates a confusion for the csv format.\n",
    "# To overpass it we replace \";\" by \".\" in essays so that we do not lose information and make the file useable.\n",
    "# b. Narrative -> N and Synopsis -> S\n",
    "# c. When author gave multiple narrative, reunite N and N2\n",
    "input_file=\"given_data_sets/TEXT.csv\"\n",
    "data0=pd.read_csv(input_file,sep='|',usecols=[0, 3, 4])\n",
    "data0=data0.replace(';','.',regex=True)\n",
    "data0=data0.replace('Narrative','N',regex=True)\n",
    "data0=data0.replace('Synopsis','S',regex=True)\n",
    "n0 = len(data0)\n",
    "# print(n0)\n",
    "for i in range(n0):\n",
    "    if data0[\"ATTRIBUTE\"][i] == \"N2\" :\n",
    "        A = data0[\"TEXT\"][i-1]\n",
    "        B = data0[\"TEXT\"][i]\n",
    "        data0[\"TEXT\"][i-1] = A+' '+B\n",
    "        data0=data0.drop([i])\n",
    "\n",
    "data0.to_csv('0_TEXT.csv',sep='|', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/urendil/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# When author gave multiple narrative, reunite N and N3\n",
    "\n",
    "data1=pd.read_csv('0_TEXT.csv',sep='|')\n",
    "n1 = len(data1)\n",
    "# print(n1)\n",
    "for i in range(n1):\n",
    "    if data1[\"ATTRIBUTE\"][i] == \"N3\" :\n",
    "        A = data1[\"TEXT\"][i-1]\n",
    "        B = data1[\"TEXT\"][i]\n",
    "        data1[\"TEXT\"][i-1] = A+' '+B\n",
    "        data1=data1.drop([i])\n",
    "data1.to_csv('1_TEXT.csv',sep='|', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dataset: 430222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/urendil/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# When author gave multiple narrative, reunite N and N4\n",
    "\n",
    "data2=pd.read_csv('1_TEXT.csv',sep='|')\n",
    "n2=len(data2)\n",
    "#print(\"Taille du dataset:\",len(data2))\n",
    "for i in range(n2):\n",
    "    if data2[\"ATTRIBUTE\"][i] == \"N4\" :\n",
    "        A = data2[\"TEXT\"][i-1]\n",
    "        B = data2[\"TEXT\"][i]\n",
    "        data2[\"TEXT\"][i-1] = A+' '+B\n",
    "        data2=data2.drop([i])\n",
    "data2.to_csv('2_TEXT.csv',sep='|', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille du dataset: 430198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/urendil/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# When author gave multiple narrative, reunite N and N5\n",
    "\n",
    "data3=pd.read_csv('2_TEXT.csv',sep='|')\n",
    "#print(\"Taille du dataset:\",len(data3))\n",
    "\n",
    "for i in range(n3):\n",
    "    if data3[\"ATTRIBUTE\"][i] == \"N5\" :\n",
    "        A = data3[\"TEXT\"][i-1]\n",
    "        B = data3[\"TEXT\"][i]\n",
    "        data3[\"TEXT\"][i-1] = A+' '+B\n",
    "        data3=data3.drop([i])\n",
    "data3.to_csv('3_TEXT.csv',sep='|', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2640\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of \"Callback\" rows in the file.\n",
    "# We won't consider Callbacks for this study as there are so few and they don't add much information.\n",
    "\n",
    "file=pd.read_csv('3_TEXT.csv',sep='|')\n",
    "n=len(file)\n",
    "S=0\n",
    "for i in range (n):\n",
    "    if file['ATTRIBUTE'][i] == 'Callback':\n",
    "        S=S+1\n",
    "#print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430196\n"
     ]
    }
   ],
   "source": [
    "# Creation of the 4_TEXT.csv file where callback rows have been removed.\n",
    "\n",
    "data4=pd.read_csv('3_TEXT.csv',sep='|')\n",
    "n4=len(data4)\n",
    "#print(n4)\n",
    "for i in range(n4):\n",
    "    if data4[\"ATTRIBUTE\"][i] == \"Callback\" :\n",
    "        data4=data4.drop([i])\n",
    "data4.to_csv('4_TEXT.csv',sep='|', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Multiple cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/urendil/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/home/urendil/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414572\n"
     ]
    }
   ],
   "source": [
    "# Cleaning of the dataset : Goal is to have the same pattern for the file, e.g. given an ITEM_ID that the two rows\n",
    "# well correspond to NARRATIVE then SYNOPSIS (if only one row : id / NaN / Content)\n",
    "\n",
    "data5=pd.read_csv('4_TEXT.csv',sep='|')\n",
    "n5=len(data5)\n",
    "#print(n5)\n",
    "i=2\n",
    "while i < n5-1:\n",
    "    \n",
    "    if data5[\"ATTRIBUTE\"][i] == \"N\" and data5[\"ATTRIBUTE\"][i+1] == \"N\" and data5[\"ITEM_ID\"][i-2] == data5[\"ITEM_ID\"][i] :\n",
    "        A = data5[\"TEXT\"][i-2]\n",
    "        B = data5[\"TEXT\"][i]\n",
    "        data5[\"TEXT\"][i-2] = A+' '+B\n",
    "        data5 = data5.drop([i])\n",
    "        i = i + 3\n",
    "    \n",
    "    elif data5[\"ATTRIBUTE\"][i] == \"N\" and data5[\"ATTRIBUTE\"][i+1] == \"N\" and data5[\"ITEM_ID\"][i-2] != data5[\"ITEM_ID\"][i] and data5[\"ITEM_ID\"][i+1] != data5[\"ITEM_ID\"][i] :\n",
    "        data5[\"ATTRIBUTE\"][i] = \"nan\"\n",
    "        i = i + 1\n",
    "    \n",
    "    else :\n",
    "        i = i + 1\n",
    "        \n",
    "#print(len(data5))\n",
    "data5.to_csv('5_TEXT.csv',sep='|', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/urendil/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/urendil/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/urendil/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413114\n"
     ]
    }
   ],
   "source": [
    "# Because of the needed i+3 in the loop of the previous code, we have to re-use the same code to treat the cases \n",
    "# that have not been treated yet.\n",
    "# note that we have ran this code multiple times to eliminate the rows skipped by the +3 iterations...\n",
    "\n",
    "data6=pd.read_csv('5_TEXT.csv',sep='|')\n",
    "n6=len(data6)\n",
    "#print(n6)\n",
    "i=2\n",
    "while i < n6-1:\n",
    "    \n",
    "    if data6[\"ATTRIBUTE\"][i] == \"N\" and data6[\"ATTRIBUTE\"][i+1] == \"N\" and data6[\"ITEM_ID\"][i-2] == data6[\"ITEM_ID\"][i] :\n",
    "        A = data6[\"TEXT\"][i-2]\n",
    "        B = data6[\"TEXT\"][i]\n",
    "        data6[\"TEXT\"][i-2] = A+' '+B\n",
    "        data6 = data6.drop([i])\n",
    "        i = i + 3\n",
    "    \n",
    "    elif data6[\"ATTRIBUTE\"][i] == \"N\" and data6[\"ATTRIBUTE\"][i+1] == \"N\" and data6[\"ITEM_ID\"][i-2] != data6[\"ITEM_ID\"][i] and data6[\"ITEM_ID\"][i+1] != data6[\"ITEM_ID\"][i] :\n",
    "        data6[\"ATTRIBUTE\"][i] = \"NaN\"\n",
    "        i = i + 1\n",
    "    \n",
    "    elif data6[\"ATTRIBUTE\"][i] == \"S\" and data6[\"ATTRIBUTE\"][i+1] == \"S\" and data6[\"ITEM_ID\"][i+1] != data6[\"ITEM_ID\"][i] :\n",
    "        data6[\"ATTRIBUTE\"][i+1] = \"NaN\"\n",
    "        i = i + 2\n",
    "    \n",
    "    else :\n",
    "        i = i + 1\n",
    "        \n",
    "# print(len(data6))\n",
    "data6.to_csv('9_TEXT.csv',sep='|', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3- Final tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413114\n",
      "Zn = 357\n",
      "Zs = 0\n",
      "[838119, 838119, 840650, 842437, 846922, 848049, 850427, 853196, 854946, 855381]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Test the quality of the dataset : are there still rows that don't respect the pattern we are looking for\n",
    "# before running THE_FILE (the cleaned one)\n",
    "\n",
    "\n",
    "data=pd.read_csv('9_TEXT.csv',sep='|')\n",
    "n=len(data)\n",
    "print(n)\n",
    "\n",
    "Zn=0\n",
    "Zs=0\n",
    "\n",
    "Ln=[]\n",
    "Ls=[]\n",
    "\n",
    "for i in range (n-1):\n",
    "    \n",
    "    if data[\"ATTRIBUTE\"][i] == \"N\" and data[\"ATTRIBUTE\"][i+1] == \"N\" :\n",
    "        #print(data[\"ITEM_ID\"][i])\n",
    "        Zn = Zn + 1\n",
    "        Ln.append(data[\"ITEM_ID\"][i])\n",
    "    \n",
    "    if data[\"ATTRIBUTE\"][i] == \"S\" and data[\"ATTRIBUTE\"][i+1] == \"S\" :\n",
    "        #print(data[\"ITEM_ID\"][i+1])\n",
    "        Zs = Zs + 1\n",
    "        Ls.append(data[\"ITEM_ID\"][i+1])\n",
    "\n",
    "print(\"Zn =\",Zn)\n",
    "print(\"Zs =\",Zs)\n",
    "print(Ln[0:10])\n",
    "print(Ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = 20\n",
      "[252110, 269574, 269701, 270963, 274331, 534517, 534869, 545509, 623021, 716570, 726891, 783998, 817578, 829157, 920827, 951293, 1090400, 1315660, 1388808, 1615449]\n"
     ]
    }
   ],
   "source": [
    "# Observation of the null value\n",
    "\n",
    "test=pd.read_csv('9_TEXT.csv',sep='|')\n",
    "\n",
    "n=len(test)\n",
    "\n",
    "Z=0\n",
    "L=[]\n",
    "\n",
    "for i in range (n-1):\n",
    "    \n",
    "    if test['ATTRIBUTE'][i] != 'N' and test['ATTRIBUTE'][i] != 'S' :\n",
    "        Z=Z+1\n",
    "        L.append(test['ITEM_ID'][i])\n",
    "\n",
    "#print(\"Z =\",Z)\n",
    "#print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Null value in the file ?\n",
    "\n",
    "test=pd.read_csv('9_TEXT.csv',sep='|')\n",
    "test.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.read_csv('9_TEXT.csv',sep='|')\n",
    "\n",
    "print(test['ATTRIBUTE'][75058])\n",
    "\n",
    "np.isnan(test['ATTRIBUTE'][75058])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Ultimate TEXT file - Ending up the preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413114\n",
      "380\n",
      "[252110, 269574, 269701, 270963, 274331, 534517, 534869, 545509, 623021, 716570, 726891, 783998, 817578, 829157, 838119, 838119, 840650, 842437, 846922, 848049, 850427, 853196, 854946, 855381, 859429, 859429, 865732, 868538, 870366, 870366, 879171, 890376, 910182, 910849, 910849, 911310, 912857, 917076, 917659, 917659, 918756, 920333, 920618, 920827, 922139, 924718, 926476, 930569, 932002, 933124, 933958, 938674, 941729, 944285, 948643, 951293, 952074, 957242, 961155, 961683, 966767, 972667, 975253, 975830, 981257, 981950, 982261, 982928, 983988, 987483, 988453, 991470, 991657, 994790, 994790, 1002555, 1002555, 1007796, 1008744, 1010715, 1013238, 1017622, 1018500, 1020720, 1024170, 1031251, 1032786, 1039409, 1042466, 1054698, 1056310, 1057181, 1058914, 1061147, 1072442, 1074684, 1085598, 1090400, 1092921, 1093775, 1096756, 1098463, 1099381, 1100689, 1101201, 1102906, 1103276, 1107149, 1108138, 1113934, 1114191, 1115865, 1118622, 1118664, 1119097, 1121478, 1122731, 1124461, 1125308, 1129277, 1130027, 1130729, 1142315, 1148931, 1151186, 1155617, 1162853, 1163218, 1170264, 1170826, 1170826, 1173861, 1174985, 1176201, 1176774, 1180324, 1184298, 1188425, 1188425, 1192249, 1194039, 1194039, 1196379, 1197077, 1210966, 1210966, 1210966, 1217879, 1219618, 1221813, 1226656, 1226824, 1227862, 1228204, 1236232, 1240967, 1243313, 1246128, 1246359, 1248284, 1249901, 1251840, 1253554, 1253680, 1254102, 1256030, 1256598, 1258215, 1262155, 1264868, 1269499, 1270323, 1270747, 1276419, 1276524, 1276621, 1283118, 1283240, 1283708, 1283723, 1285328, 1291916, 1291916, 1293168, 1293968, 1295889, 1297511, 1300604, 1300861, 1302297, 1303795, 1309598, 1311568, 1311666, 1315354, 1315660, 1318200, 1318941, 1319154, 1320591, 1321243, 1321243, 1322299, 1325771, 1333651, 1333786, 1339142, 1339142, 1339142, 1339142, 1340550, 1340965, 1341434, 1343844, 1344089, 1345258, 1345304, 1345355, 1345494, 1347051, 1347767, 1348051, 1351571, 1351591, 1353378, 1353804, 1355192, 1355703, 1359031, 1361695, 1362312, 1363111, 1364006, 1365077, 1366152, 1371915, 1380317, 1381644, 1381644, 1382570, 1383286, 1384255, 1384506, 1385414, 1385957, 1386256, 1386256, 1388032, 1388043, 1388808, 1388808, 1393474, 1397559, 1405790, 1413262, 1413387, 1414841, 1416828, 1418250, 1419222, 1419225, 1419225, 1420388, 1422636, 1425265, 1425627, 1429772, 1429776, 1430284, 1430484, 1430527, 1430618, 1430818, 1432055, 1437259, 1437759, 1440791, 1441287, 1441867, 1445468, 1445533, 1448616, 1452448, 1455155, 1455164, 1457262, 1457268, 1457420, 1462762, 1463032, 1463032, 1463507, 1466756, 1468249, 1469468, 1470479, 1471241, 1473875, 1473875, 1473949, 1480225, 1480288, 1480969, 1482344, 1485859, 1486003, 1490282, 1490741, 1497293, 1497357, 1497442, 1499496, 1499496, 1505252, 1506325, 1506325, 1512063, 1513280, 1514691, 1515818, 1515849, 1518403, 1518673, 1531862, 1538007, 1538032, 1539500, 1542770, 1544926, 1545229, 1554933, 1559489, 1559538, 1572981, 1574775, 1578005, 1578357, 1579491, 1579587, 1580761, 1580761, 1583960, 1584048, 1585950, 1588336, 1589725, 1592035, 1592035, 1592693, 1595945, 1600130, 1600193, 1601030, 1604953, 1607360, 1609004, 1609471, 1609640, 1611578, 1612334, 1614161, 1615449, 1617968, 1619989, 1622900, 1624788, 1634156, 1645399, 1645797, 1650001, 1650001, 1650622, 1660606, 1660606, 1668705, 1670685, 1671847, 1673187, 1674530, 1676210]\n",
      "206747\n",
      "10102.172346830368\n"
     ]
    }
   ],
   "source": [
    "# Final modification for the text file : create one with columns ITEM_ID / TITLE (Synopsis or NaN) / CONTENT (Narrative)\n",
    "tic=time.time()\n",
    "\n",
    "new = pd.DataFrame(columns=['ITEM_ID', 'TITLE', 'CONTENT'])\n",
    "old = pd.read_csv('9_TEXT.csv',sep='|')\n",
    "\n",
    "l=len(old)\n",
    "#print(l)\n",
    "\n",
    "i=0\n",
    "L=[]\n",
    "S=0\n",
    "\n",
    "while i < l-1:\n",
    "    \n",
    "    if old['ATTRIBUTE'][i] == 'N' and old['ATTRIBUTE'][i+1] == 'S' and old['ITEM_ID'][i] == old['ITEM_ID'][i+1] :\n",
    "        new.loc[i] = [old['ITEM_ID'][i], old['TEXT'][i+1], old['TEXT'][i]]\n",
    "        i=i+2\n",
    "    \n",
    "    else :\n",
    "        new.loc[i] = [old['ITEM_ID'][i], old['ATTRIBUTE'][i], old['TEXT'][i]]\n",
    "        L.append(old['ITEM_ID'][i])\n",
    "        S=S+1\n",
    "        i=i+1\n",
    "\n",
    "#print(S)\n",
    "#print(L)\n",
    "#print(len(new))\n",
    "\n",
    "tac=time.time()-tic\n",
    "print(tac)\n",
    "\n",
    "new.to_csv('sext.csv',sep='|', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/urendil/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/urendil/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/urendil/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Final touch to finally build new_TEXT.csv\n",
    "\n",
    "data=pd.read_csv('sext.csv',sep='|')\n",
    "\n",
    "n=len(data)\n",
    "i=100000\n",
    "\n",
    "while i < n-1 :\n",
    "    \n",
    "    if data['ITEM_ID'][i] == 534869 :\n",
    "        data=data.drop([i])\n",
    "        i=i+50000\n",
    "    \n",
    "    if data['ITEM_ID'][i] == 1283708 :\n",
    "        A = data[\"CONTENT\"][i]\n",
    "        B = data[\"CONTENT\"][i+1]\n",
    "        data[\"CONTENT\"][i] = A+' '+B\n",
    "        data=data.drop([i+1])\n",
    "        i=i+2\n",
    "    \n",
    "    if data['ITEM_ID'][i] == 1283723 :\n",
    "        data['TITLE'][i] = 'NaN'\n",
    "        i=i+1\n",
    "    \n",
    "    if data['ITEM_ID'][i] == 1388808 :\n",
    "        data['TITLE'][i] = 'NaN'\n",
    "        data=data.drop([i+1])\n",
    "        i=n\n",
    "    \n",
    "    else :\n",
    "        i=i+1\n",
    "\n",
    "data.to_csv('new_TEXT.csv',sep='|', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206744\n"
     ]
    }
   ],
   "source": [
    "# print(len(pd.read_csv('new_TEXT.csv',sep='|')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Pre-processing of ALL_ITEMS.csv\n",
    "\n",
    "1. TEXT : items come from number 75 085 to 1 690 771 and there are only 2 rows per items (more if many writers)\n",
    "2. ALL_ITEMS : items come from number 75 085 to 1 690 771 and approximately 50 rows per items\n",
    "\n",
    "Creation of a file untitled new_ALL_ITEMS.csv with the following columns ['ITEM_ID', 'DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the item_id and date information from the ALL_ITEMS file\n",
    "input_file1=\"given_data_sets/ALL_ITEMS.csv\"\n",
    "data=pd.read_csv(input_file1,sep='|',usecols=[0, 3, 4])\n",
    "\n",
    "n=len(data)\n",
    "L=[['ITEM_ID', 'DATE']]\n",
    "\n",
    "for i in range(n) :\n",
    "    if data[\"ATTRIBUTE\"][i] == \"Date\" :\n",
    "        L.append([data[\"ITEM_ID\"][i], data[\"VALUE\"][i]])\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('1_ALL_ITEMS.csv','w', newline='') as fp :\n",
    "    a = csv.writer(fp, delimiter = '|')\n",
    "    a.writerows(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206389\n",
      "206387\n"
     ]
    }
   ],
   "source": [
    "# Make sure that both files (text and time) have the same size, so that we can unify them\n",
    "\n",
    "old=pd.read_csv('1_ALL_ITEMS.csv',sep='|')\n",
    "\n",
    "n=len(old)\n",
    "# print(n)\n",
    "i=0\n",
    "\n",
    "for i in range(n-1):\n",
    "    \n",
    "    if old['ITEM_ID'][i] == 534869:\n",
    "        old=old.drop([i])\n",
    "    \n",
    "    elif old['ITEM_ID'][i] == 953498:\n",
    "        old=old.drop([i])\n",
    "\n",
    "# print(len(old))\n",
    "old.to_csv('new_ALL_ITEMS.csv',sep='|', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Unification of both previous files\n",
    "\n",
    "Goal is to create a single DataFrame with the following structure :\n",
    "['TITLE', 'CONTENT', 'TIME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat THE_FILE\n",
    "# To do so, first add a new column to the file, put the right data in its row and delete the item_id column\n",
    "\n",
    "text = pd.read_csv('new_TEXT.csv',sep='|', usecols=[1, 2]) # item_id / title(synopsis) / content(narrative)\n",
    "item = pd.read_csv('new_ALL_ITEMS.csv',sep='|', usecols=[1]) # item_id / date\n",
    "\n",
    "text['TIME'] = item['DATE']\n",
    "\n",
    "text.to_csv('data.csv',sep='|', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- A file useable within SumUp's Nucleus\n",
    "Just turn the separators of the file from '|' to ','."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this file, we kept the commas of the texts.\n",
    "\n",
    "df=pd.read_csv('data.csv',sep='|')\n",
    "df.to_csv('ASRS0.csv',sep=',',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this file we removed the commas from the texts.\n",
    "\n",
    "df=pd.read_csv('data.csv',sep='|')\n",
    "df=df.replace(',',' ',regex=True)\n",
    "df.to_csv('ASRS.csv',sep=',',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 41- Right date format and column order for nucleus requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treating the 4 cases of the dataset where date format is yyyy and not yyyymm\n",
    "import pandas as pd\n",
    "data=pd.read_csv('ASRS0.csv',sep=',')\n",
    "\n",
    "n=len(data['TIME'])\n",
    "for i in range(n):\n",
    "    if data['TIME'][i] < 3000 :\n",
    "        # print(data['TIME'][i])\n",
    "        data['TIME'][i] = data['TIME'][i-1]\n",
    "        # print(data['TIME'][i])\n",
    "\n",
    "data.to_csv('ASRS0.csv',sep=',',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('ASRS0.csv',sep=',')\n",
    "\n",
    "# columns : from TITLE, CONTENT, TIME to time, content, title\n",
    "A=data['TITLE']\n",
    "#print('1')\n",
    "B=data['TIME']\n",
    "#print('2')\n",
    "data['TITLE']=B\n",
    "#print('3')\n",
    "data['TIME']=A\n",
    "#print('4')\n",
    "data=data.rename(columns={\"TITLE\": \"time\", \"CONTENT\": \"content\", \"TIME\": \"title\"})\n",
    "#print('5')\n",
    "\n",
    "# dates : from yyyymm to yyyy-mm-01\n",
    "d='01'\n",
    "data['time'] = pd.to_datetime(data['time'], format='%Y%m').dt.strftime(\"%Y-%m-%d\")\n",
    "#print('6')\n",
    "\n",
    "\n",
    "data.to_csv('ASRS1.csv',sep=',',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
